![image](https://github.com/user-attachments/assets/31c65cd5-840d-4924-b40d-011f23c9553d)


# **DeepLense Classification Task**

Hello! This repository contains my solution for the **ML4SCI DeepLense classification challenge**, which involves identifying **No Substructure, Subhalo, and Vortex** in lensing images. I developed two models:  

- **BaselineCNN** (a standard CNN model without physics-based enhancements.)  
- **LensVIT_PINN** (my custom model, combining Vision Transformers with ResNet)  

## **Approach**  
First, I built the **BaselineCNN** model, which achieved **AUC scores of 0.9926, 0.9810, and 0.9933** across the three classes. My goal was to build a model that could **match or surpass** this baseline.  

I developed **LensVIT_PINN**, integrating a **Vision Transformer (ViT) with ResNet**. Initially, I explored a **physics-informed approach with gradient preprocessing**, but it resulted in an **AUC of only 0.8786**. Simplifying the model led to a significant improvement.  

## **Results**  

| Model           | No Substructure (AUC) | Subhalo (AUC) | Vortex (AUC) |
|---------------|-------------------|--------------|-------------|
| **LensVIT_PINN** | **0.9902** | **0.9804** | **0.9932** |
| **BaselineCNN** | **0.9926** | **0.9810** | **0.9933** |

LensVIT_PINN delivers competitive performance, closely matching the **BaselineCNN**, with slight variations across categories.  

## **Conclusion**  
LensVIT_PINN demonstrates **strong classification performance**, confirming that a transformer-based architecture can be a viable alternative for lensing image classification. Thereâ€™s still room for **further optimization**, especially in refining the physics-informed approach.  

